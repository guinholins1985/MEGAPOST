
# Intelligent AI Gateway for Google Gemini

This project implements a robust, scalable API Gateway in Python using FastAPI to manage a pool of Google AI Studio (Gemini) API keys. It provides a single, resilient endpoint for your applications, handling load balancing, rate limiting, and automatic failover internally.

## Features

-   **API Gateway/Proxy**: A single endpoint (`/v1/generate`) to route all AI requests.
-   **Round-Robin Load Balancing**: Distributes requests evenly across all configured API keys.
-   **Dynamic Rate Limiting**: Monitors requests per minute for each key and temporarily rests keys that hit their limit.
-   **Seamless Quota Fallback**: Automatically detects quota-exhausted keys (e.g., daily limit) and retries requests with the next available key in the pool.
-   **Real-time Monitoring**: Detailed logging for requests, successes, failures, and failover events.
-   **Redis Caching**: Caches responses to identical prompts to reduce latency and save on API costs.
-   **Client Authentication**: Protects the gateway with a simple Bearer token authentication scheme.
-   **Graceful Fallback**: Returns a user-friendly `503 Service Unavailable` message if all keys in the pool are simultaneously unavailable.
-   **Scalability**: Easily add or remove Google AI API keys by updating a single environment variable.
-   **Automatic API Documentation**: Interactive API documentation (Swagger UI and ReDoc) is automatically generated by FastAPI.

## Tech Stack

-   **Backend**: FastAPI (Python)
-   **Server**: Uvicorn
-   **Caching**: Redis
-   **AI Model**: Google Gemini

---

## üöÄ Getting Started

### 1. Prerequisites

-   Python 3.8+
-   Redis instance (running locally or on a cloud service)
-   A `.env` file for your environment variables.

### 2. Installation

Clone the repository and install the required Python packages.

```bash
git clone <your-repo-url>
cd <your-repo-folder>
pip install -r api_gateway/requirements.txt
```

### 3. Configuration

Create a file named `.env` in the root directory of the project and add the following variables.

```env
# A comma-separated list of your 20 Google AI Studio API keys
# IMPORTANT: Do not add spaces between the keys
GEMINI_API_KEYS="key1,key2,key3,..."

# A comma-separated list of secret keys for your client applications
# Format: "client_key1:ClientName1,client_key2:ClientName2"
CLIENT_API_KEYS_MAP="your-secret-client-key-1:ClientApp1,your-secret-client-key-2:WebApp2"

# The connection URL for your Redis instance
REDIS_URL="redis://localhost:6379"
```

The `main.py` file is configured to read these variables, but you will need to modify it to parse them from the environment. For this example, they are hardcoded but it's strongly recommended to use environment variables.

### 4. Running Locally

You can run the application using Uvicorn.

```bash
# From the root directory
uvicorn api_gateway.main:app --reload
```

The API will be available at `http://127.0.0.1:8000`.

-   **Swagger UI (Interactive Docs)**: `http://127.0.0.1:8000/docs`
-   **ReDoc (Alternative Docs)**: `http://127.0.0.1:8000/redoc`

### 5. Making a Request

Use a tool like `curl` or Postman to send a request to the `/v1/generate` endpoint.

```bash
curl -X POST "http://127.0.0.1:8000/v1/generate" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer your-secret-client-key-1" \
-d '{
  "prompt": "Explain the theory of relativity in simple terms.",
  "model": "gemini-1.5-flash"
}'
```

---

## ‚òÅÔ∏è Deployment Instructions

### Option 1: Deploying to Heroku

1.  **Create a `Procfile`**: In the root directory, create a file named `Procfile` with the following content:
    ```
    web: uvicorn api_gateway.main:app --host 0.0.0.0 --port $PORT
    ```

2.  **Login to Heroku and Create an App**:
    ```bash
    heroku login
    heroku create your-gateway-app-name
    ```

3.  **Add a Redis Add-on**:
    ```bash
    heroku addons:create heroku-redis:hobby-dev -a your-gateway-app-name
    ```
    This will automatically set the `REDIS_URL` config var.

4.  **Set Config Vars**: Set your API keys in Heroku's config vars (do not commit them).
    ```bash
    heroku config:set GEMINI_API_KEYS="key1,key2,..." -a your-gateway-app-name
    heroku config:set CLIENT_API_KEYS_MAP="key:name,..." -a your-gateway-app-name
    ```

5.  **Deploy**:
    ```bash
    git push heroku main
    ```

### Option 2: Deploying to Google Cloud Run

1.  **Create a `Dockerfile`**: In the root directory, create a file named `Dockerfile`:
    ```Dockerfile
    FROM python:3.9-slim

    WORKDIR /app

    COPY api_gateway/requirements.txt .
    RUN pip install --no-cache-dir -r requirements.txt

    COPY ./api_gateway /app/api_gateway

    CMD ["uvicorn", "api_gateway.main:app", "--host", "0.0.0.0", "--port", "8080"]
    ```

2.  **Enable Google Cloud Services**: Make sure you have the gcloud CLI installed and have enabled Cloud Build, Cloud Run, and Artifact Registry APIs.

3.  **Build and Push the Docker Image**:
    ```bash
    gcloud builds submit --tag gcr.io/YOUR_PROJECT_ID/ai-gateway
    ```

4.  **Deploy to Cloud Run**: Deploy the container, setting the necessary environment variables.
    ```bash
    gcloud run deploy ai-gateway-service \
      --image gcr.io/YOUR_PROJECT_ID/ai-gateway \
      --platform managed \
      --region YOUR_REGION \
      --allow-unauthenticated \
      --set-env-vars "GEMINI_API_KEYS=key1,key2,..." \
      --set-env-vars "CLIENT_API_KEYS_MAP=key:name,..." \
      --set-env-vars "REDIS_URL=redis://YOUR_REDIS_IP:PORT"
    ```
    *Note: For Redis, you should set up a Memorystore instance and connect your Cloud Run service to it via a Serverless VPC Access connector for security.*
```
